{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878b5135",
   "metadata": {},
   "source": [
    "# Intro to Game AI and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ab8a2",
   "metadata": {},
   "source": [
    "* Bu notebook kapsamında, kendi kendine oyun oynayan ajanlar oluşturacağız.\n",
    "* Oyun ortamımızı oluşturacağız, ajanımızı oluşturacağız ve yapay zeka temelli oyunlar oluşturmak için geleneksel yöntemlere bakacağız. Öyle ki oluşturduğumuz ajanlar bir çok acemi oyuncuyu yenecek. :)\n",
    "* Bölüm - 4 te ise Reinforcement Learning alanında en yeni algoritmaları deneyeceğiz.\n",
    "* Hazırsanız başlayın !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ortamımızı kurarken kaggle'dan yararlanacağız. İlk olarak anaconda prompt üzerinde pip install kaggle_environments diyerek\n",
    "# paketi indirmek gerek. Sonrasında import edelim.\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "\n",
    "# Oyun ortamımızı create edelim.\n",
    "env = make(\"connectx\", debug=True)\n",
    "\n",
    "# Kullanılabilir varsayılan araçlarımız.\n",
    "print(list(env.agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İki rastgele ajan bir tur oyun oynuyor.\n",
    "env.run([\"random\", \"random\"])\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f68ae",
   "metadata": {},
   "source": [
    "**Agent'ları Tanımlayalım.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e163c9",
   "metadata": {},
   "source": [
    "* Agent'ımızı iki bağımsız değişkeni kabul eden bir fonksiyon olarak tanımlayacağız.\"obs ve config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# baştaki sütunu seç.\n",
    "def agent_random(obs, config):\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    return random.choice(valid_moves)\n",
    "\n",
    "# orta sütunu seç.\n",
    "def agent_middle(obs, config):\n",
    "    return config.columns//2\n",
    "\n",
    "# en son da ki sütunu seç.\n",
    "def agent_leftmost(obs, config):\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    return valid_moves[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288fe23a",
   "metadata": {},
   "source": [
    "* **obs:** iki bilgi içerir. \n",
    "* *obs.board:* her ızgara konumu için yani 6*7 lik matrisin her elemanı için bir öge içeren bir python listesi.\n",
    "* *obs.mark:* agent'a atanan parça \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ee269",
   "metadata": {},
   "source": [
    "* Yani işin özü şudur. oyun üzerinde anlık olarak bulunan ajanlardan mavi olanlara 1, sarı olanlara 2 değerini ata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d5c17",
   "metadata": {},
   "source": [
    "* **config** 3 adet bilgi içerir. \n",
    "* *config.columns: 7 kolon* , *config.rows: 6 satır*, *config.in a row: oyunu kazanmak için üst üste alınacak parça sayısı--> 4*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f4dae",
   "metadata": {},
   "source": [
    "**Agent'ları Değerlendirelim.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajanlar bir oyun turu oynar.\n",
    "env.run([agent_leftmost, agent_random])\n",
    "\n",
    "# gösterelim.\n",
    "env.render(mode=\"ipython\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320756f",
   "metadata": {},
   "source": [
    "Tek bir oyunun sonucu genellikle ajanlarımızın ne kadar iyi performans göstereceğini anlamak için yeterli bilgi değildir. Birden fazla oyunda her bir agent'ın kazanma yüzdelerini hesaplayacağız. Bunu yapmak için get_win_percentages() fonksiyonunu kullanacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d826b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # varsayılan connect four kurulumunu kullanıyoruz.\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 zaman ayarlaması.\n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 zaman ayarlaması.      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Kazanma Yüzdesi:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Kazanma Yüzdesi:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Agent 1 için Geçersiz Oyun Sayısı:\", outcomes.count([None, 0]))\n",
    "    print(\"Agent 2 için Geçersiz Oyun Sayısı:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rastgele oynayan agenta göre hangi agent'ımız daha fazla oyun kazanmış. bakalım!\n",
    "\n",
    "get_win_percentages(agent1=agent_middle, agent2=agent_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent_leftmost, agent2=agent_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7cc36",
   "metadata": {},
   "source": [
    "Şimdi ajanımıza oyun oynamayı öğretmeye başlayalım. Her olası geçerli hamleye bir puan atamak için sezgiselliği kullanacağız ve\n",
    "en yüksek puanı alan hareketi seçeceğiz. (Birden fazla hamle yüksek puanı alırsa, rastgele birini seçeriz.)**One-step lookahead** ajanın sadece bir hamle uzağı hesapladığını ifade eder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ajan seçilen sütuna bir parça düşürürse puan hesaplanır.\n",
    "def score_move(grid, col, mark, config):\n",
    "    next_grid = drop_piece(grid, col, mark, config)\n",
    "    score = get_heuristic(next_grid, mark, config)\n",
    "    return score\n",
    "\n",
    "# Score_move için yardımcı işlev: ajan seçilen sütuna bir parça düşürürse bir sonraki adımda tahtayı alır.\n",
    "def drop_piece(grid, col, mark, config):\n",
    "    next_grid = grid.copy()\n",
    "    for row in range(config.rows-1, -1, -1):\n",
    "        if next_grid[row][col] == 0:\n",
    "            break\n",
    "    next_grid[row][col] = mark\n",
    "    return next_grid\n",
    "\n",
    "# Score_move için yardımcı işlev: ızgara için sezgisel değerini hesaplar.\n",
    "def get_heuristic(grid, mark, config):\n",
    "    num_threes = count_windows(grid, 3, mark, config)\n",
    "    num_fours = count_windows(grid, 4, mark, config)\n",
    "    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n",
    "    score = num_threes - 1e2*num_threes_opp + 1e6*num_fours\n",
    "    return score\n",
    "\n",
    "# Get_heuristic için yardımcı işlev: pencerenin sezgisel koşulları karşılayıp karşılamadığını kontrol eder\n",
    "def check_window(window, num_discs, piece, config):\n",
    "    return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n",
    "    \n",
    "# Get_heuristic için yardımcı işlev: belirtilen sezgisel koşulları karşılayan pencere sayısını sayar\n",
    "def count_windows(grid, num_discs, piece, config):\n",
    "    num_windows = 0\n",
    "    # horizontal\n",
    "    for row in range(config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[row, col:col+config.inarow])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # vertical\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns):\n",
    "            window = list(grid[row:row+config.inarow, col])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # positive diagonal\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # negative diagonal\n",
    "    for row in range(config.inarow-1, config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    return num_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-step lookahead tanımlanması\n",
    "\n",
    "# Ajan her zaman iki bağımsız değişkeni kabul eden bir Python işlevi olarak uygulanır: obs ve config. \n",
    "# Bu işlemi yukarda yaptık zaten.\n",
    "def agent(obs, config):\n",
    "    # Geçerli hamlelerin listesini alın.\n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    # Tahtayı 2D ızgaraya dönüştürün.\n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    # Bir sonraki dönüşte olası her tahtaya bir puan atamak için sezgisel kullanın.\n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config) for col in valid_moves]))\n",
    "    # Sezgisel değeri en üst düzeye çıkaran sütunların (hareketlerin) bir listesini alın.\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    # Sütunlardan rastgele seçelim.\n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875dc18",
   "metadata": {},
   "source": [
    "*Yukarıdaki kodu yorumlayalım**\n",
    "\n",
    "* Agent geçerli hareketlerin bir listesini alarak başlar.\n",
    "* Ardından oyun tahtasını 2D numpy dizisine dönüştürüyoruz.\n",
    "* Sonra score_move() işlevi, geçerli her hareket için sezgisel değer hesaplar.\n",
    "* drop_piece(), oynatıcı diskini seçilen sütuna bıraktığında oluşan ızgarayı döndürür.\n",
    "* get_heuristic(), sağlanan kartın (grid) sezgisel değerini hesaplar; burada işaret, agent'ın işaretidir. Bu işlev, buluşsaldan belirli koşulları karşılayan pencere sayısını (satır, sütun veya köşegendeki dört bitişik konumdan) sayan count_windows() işlevini kullanır. Özellikle, count_windows (grid, num_discs, piece, config), oyun tahtasındaki (grid), mark piece ile oynatıcıdan (agent veya rakip) num_discs parçaları içeren ve pencerede kalan konumların boş olduğu pencere sayısını verir. Örneğin,\n",
    "* num_dıscs =4 ve piece = obs ayarı.mark, ajanın üst üste dört disk alma sayısını sayar.\n",
    "* num_dıscs = 3 ve piece = obs ayarı.%2+1 işareti, rakibin üç diski olduğu ve kalan konumun boş olduğu pencere sayısını sayar (rakip boş noktayı doldurarak kazanır).\n",
    "* Son olarak, sezgisel olanı en üst düzeye çıkaran ve rastgele bir tane (tekdüze) seçen sütunların listesini alırız.\n",
    "\n",
    "* (Not: Yukarıdaki kodu anlamak için zaman ayırdıktan sonra, çok daha hızlı çalışmasını sağlamak için nasıl yeniden yazacağınızı görebiliyor musunuz? İpucu olarak, count_windows() işlevinin oyun tahtasındaki konumların üzerinde döngü yapmak için birkaç kez kullanıldığını unutmayın.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ceb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make, evaluate\n",
    "\n",
    "env = make(\"connectx\")\n",
    "\n",
    "env.run([agent, \"random\"])\n",
    "\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performanslara göz atalım.\n",
    "\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "   \n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "           \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "        \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Kazanma:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Kazanma:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Agent 1 Geçersiz:\", outcomes.count([None, 0]))\n",
    "    print(\"Agent 2 Geçersiz:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent, agent2=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256e4d9",
   "metadata": {},
   "source": [
    "Şimdi ise agent'ımızın başarısını önemli ölçüde artıracak olan oyun teorisindeki min-max işlevini kullanacağız. **Minimax** şu şekilde çalışır.Oyun ağacının derinliklerinden gelen bilgileri kullanmak istiyoruz. Şimdilik 3 derinlikte çalıştığımızı varsayalım. Bu şekilde, ajan hareketine karar verirken, ajan tüm olası oyun tahtalarını dikkate alır. Bu durumda ajan kendi hamlesini, rakibin olası hamlelerini ve bir sonraki kendi hamlesini hesaplar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e27d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# önceki kodun birebir aynısını aldık.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def drop_piece(grid, col, mark, config):\n",
    "    next_grid = grid.copy()\n",
    "    for row in range(config.rows-1, -1, -1):\n",
    "        if next_grid[row][col] == 0:\n",
    "            break\n",
    "    next_grid[row][col] = mark\n",
    "    return next_grid\n",
    "\n",
    "\n",
    "def check_window(window, num_discs, piece, config):\n",
    "    return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n",
    "    \n",
    "\n",
    "def count_windows(grid, num_discs, piece, config):\n",
    "    num_windows = 0\n",
    "    # horizontal\n",
    "    for row in range(config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[row, col:col+config.inarow])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # vertical\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns):\n",
    "            window = list(grid[row:row+config.inarow, col])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # positive diagonal\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # negative diagonal\n",
    "    for row in range(config.inarow-1, config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    return num_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438085db",
   "metadata": {},
   "source": [
    "* Ayrıca, önceki bölümden sezgiselliği değiştirmemiz gerekecek, çünkü rakip artık oyun tahtasını değiştirebiliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064972b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimax için yardımcı işlev: oyun alanı için sezgisel değer hesaplar.\n",
    "def get_heuristic(grid, mark, config):\n",
    "    num_threes = count_windows(grid, 3, mark, config)\n",
    "    num_fours = count_windows(grid, 4, mark, config)\n",
    "    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n",
    "    num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n",
    "    score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cda2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimax için ihtiyaç duyacağımız birkaç ek işlev.\n",
    "\n",
    "# Uses minimax to calculate value of dropping piece in selected column\n",
    "def score_move(grid, col, mark, config, nsteps):\n",
    "    next_grid = drop_piece(grid, col, mark, config)\n",
    "    score = minimax(next_grid, nsteps-1, False, mark, config)\n",
    "    return score\n",
    "\n",
    "# Helper function for minimax: checks if agent or opponent has four in a row in the window\n",
    "def is_terminal_window(window, config):\n",
    "    return window.count(1) == config.inarow or window.count(2) == config.inarow\n",
    "\n",
    "# Helper function for minimax: checks if game has ended\n",
    "def is_terminal_node(grid, config):\n",
    "    # Check for draw \n",
    "    if list(grid[0, :]).count(0) == 0:\n",
    "        return True\n",
    "    # Check for win: horizontal, vertical, or diagonal\n",
    "    # horizontal \n",
    "    for row in range(config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[row, col:col+config.inarow])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # vertical\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns):\n",
    "            window = list(grid[row:row+config.inarow, col])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # positive diagonal\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # negative diagonal\n",
    "    for row in range(config.inarow-1, config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Minimax implementation\n",
    "def minimax(node, depth, maximizingPlayer, mark, config):\n",
    "    is_terminal = is_terminal_node(node, config)\n",
    "    valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n",
    "    if depth == 0 or is_terminal:\n",
    "        return get_heuristic(node, mark, config)\n",
    "    if maximizingPlayer:\n",
    "        value = -np.Inf\n",
    "        for col in valid_moves:\n",
    "            child = drop_piece(node, col, mark, config)\n",
    "            value = max(value, minimax(child, depth-1, False, mark, config))\n",
    "        return value\n",
    "    else:\n",
    "        value = np.Inf\n",
    "        for col in valid_moves:\n",
    "            child = drop_piece(node, col, mark%2+1, config)\n",
    "            value = min(value, minimax(child, depth-1, True, mark, config))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_STEPS = 3 # agent'a 3 hamle düşün dedik. Ne kadar derin , o kadar süre :d\n",
    "\n",
    "# agent tanımlamasını bir önceki bölümlerde bir kaç kez yaptık.\n",
    "def agent(obs, config):\n",
    "    \n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    \n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    \n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n",
    "    \n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    \n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oyun zamanı .\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "\n",
    "env = make(\"connectx\")\n",
    "\n",
    "env.run([agent, \"random\"])\n",
    "\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04381ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performans hesabı.\n",
    "\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    \n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "             \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "          \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Kazanma:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Kazanma:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Agent 1 Kaybetme:\", outcomes.count([None, 0]))\n",
    "    print(\"Agent 2 Kaybetme:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16704978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 oyun için hesaplayalım.\n",
    "\n",
    "get_win_percentages(agent1=agent, agent2=\"random\", n_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06c958",
   "metadata": {},
   "source": [
    "Şimdi ise **Deep Reinforcement Learning** konusuna bakacağız.Sezgisellik kullanmadan akıllı bir ajan oluşturmak için takviye öğrenimini nasıl kullanacağınızı öğreneceğiz. Sadece oyunu oynayarak ve kazanma oranını en üst düzeye çıkarmaya çalışarak, zaman içinde ajanın stratejisini kademeli olarak geliştireceğiz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc58bb7",
   "metadata": {},
   "source": [
    "* Derin sinir ağımız (DNN) geçerli oyun tahtasındaki dizilimi girdi kabul eder ve olası her hareket için bir olasılık verir.\n",
    "* Agent bu olasıklıklardan bir seçim yapar. Yüksek olasılığın seçimi daha yüksektir.\n",
    "* Bu şekilde, iyi bir oyun stratejisini kodlamak için, sadece ağın ağırlıklarını değiştirmemiz gerekir, böylece mümkün olan her oyun tahtası için daha iyi hamlelere daha yüksek olasılıklar atar.( derin öğrenme )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba89606",
   "metadata": {},
   "source": [
    "**Peki soru şu? Ağın ağırlıklarını nasıl güncelleriz ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5bc1c",
   "metadata": {},
   "source": [
    "* Her hareketten sonra, ajana ne kadar iyi yaptığını söyleyen bir ödül veriyoruz.\n",
    "* Eğer ajan bu hamlede oyunu kazanırsa, ona +1'lik bir ödül veririz.\n",
    "* Aksi takdirde, ajan geçersiz bir hamle yaparsa (oyunu bitirir), ona -10'luk bir ödül veririz.\n",
    "* Rakip bir sonraki hamlesinde oyunu kazanırsa (yani, ajan rakibinin kazanmasını engelleyemedi), ajana -1'lik bir ödül veririz.\n",
    "* Aksi takdirde, ajan 1/42'lik bir ödül alır.(oyun tahtası 6*7 lik.)\n",
    "* Her oyunun sonunda, ajan ödülünü ekler. Ödüllerin toplamını ajanın kümülatif ödülü olarak adlandırıyoruz.\n",
    "* **Örneğin:** \n",
    "* Oyun 8 hamle sürdüyse (her oyuncu dört kez oynadı) ve ajan nihayetinde kazanırsa, kümülatif ödülü 3 * (1/42) + 1'dir. \n",
    "* Oyun 11 hamle sürdüyse (ve rakip ilk önce oynadı, böylece ajan beş kez oynadı) ve rakip son hamlesinde kazandıysa, ajanın kümülatif ödülü 4 * (1/42) - 1'dir.\n",
    "* Oyun berabere biterse, ajan tam olarak 21 hamle oynadı ve 21 * (1/42) kümülatif ödül aldı.\n",
    "* Oyun 7 hamle sürdüyse ve ajan geçersiz bir hamle seçtiğinde sona ererse, ajan 3 * (1/42) - 10 kümülatif ödül alır.\n",
    "* **AMACIMIZ:** Ortalama olarak ajanın kümülatif ödülünü en üst düzeye çıkaran sinir ağının ağırlıklarını bulmaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbe7b8",
   "metadata": {},
   "source": [
    "Bir ajanın performansını izlemek için ödül kullanma fikri, **takviye öğrenimi - reinforcement learning** alanında temel bir fikirdir. Sorunu bu şekilde tanımladıktan sonra, bir aracı üretmek için çeşitli takviye öğrenme algoritmalarından herhangi birini kullanabiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863a6c8",
   "metadata": {},
   "source": [
    "**Reinforcement Learning / Takviyeli Öğrenme**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fef122",
   "metadata": {},
   "source": [
    "**DQN**, **A2C** ve **PPO** gibi birçok farklı **takviye öğrenme algoritması** vardır. Tüm bu algoritmalar bir agent üretmek için benzer bir işlem kullanır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e122c",
   "metadata": {},
   "source": [
    "* Başlangıçta, ağırlıklar rasgele değerlere ayarlanır.\n",
    "* Agent oyunu oynarken algoritma, kümülatif ödülün ortalama olarak nasıl etkilendiğini görmek için ağırlıklar için sürekli olarak yeni değerler dener. Zamanla, birçok oyun oynadıktan sonra, ağırlıkların kümülatif ödülü nasıl etkilediğine dair iyi bir fikir edinir ve algoritma daha iyi performans gösteren ağırlıklara doğru yerleşir.\n",
    "* Bu şekilde, oyunu kazanmaya çalışan bir ajan ile ortaya çıkar. (böylece +1'in son ödülünü alır ve -1 ve -10'dan kaçınır) ve oyunu mümkün olduğunca uzun süre sürdürmeye çalışır (böylece 1/42 bonusunu toplayabilir)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb4527",
   "metadata": {},
   "source": [
    "**Uygulama**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1e28e",
   "metadata": {},
   "source": [
    "PPO algoritmasını kullanacağız. Tensorflow 1 ile devam edeceğiz. Uygualama tensorflow 2 yi desteklememekte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0610f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15.0\n",
    "!pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb26924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check version of tensorflow\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "class ConnectFourGym:\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(self.rows,self.columns,1), dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e9bc4",
   "metadata": {},
   "source": [
    " **Yukarıdaki kod ne yapar?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df59e7",
   "metadata": {},
   "source": [
    "ConnectFourGym sınıfı connectx'i OpenAI GYM ortamı olarak uygular ve çeşitli yöntemler kullanır:\n",
    "* reset() : her oyunun başında çağrılır. başlangıç oyun tahtasını 6 satır ve 7 sütun içeren bir 2D numpy dizisi olarak döndürür.\n",
    "* change_reward() : agentın aldığı ödülleri özelleştirir. (Yarışmanın, agentları sıralamak için kullanılan ödüller için zaten kendi sistemi vardır ve bu yöntem, değerleri tasarladığımız ödül sistemine uyacak şekilde değiştirir.)\n",
    "* step(), rakibin tepkisi ile birlikte ajanın eylem seçimi için kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefb8ce",
   "metadata": {},
   "source": [
    "Şimdi rastgele ajanı yenmek için bir ajan yetiştireceğiz. Bu rakibi aşağıdaki agent2 argümanında belirtiyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ConnectFourGym(agent2=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e22fb0",
   "metadata": {},
   "source": [
    "* Kararlı Taban Çizgileri oyunu, \"vektörize\" ortamlarla çalışmamızı gerektirir. Bunun için Dummyvevent sınıfını kullanabiliriz.\n",
    "* Monitor sınıfı, gittikçe daha fazla oyun oynarken aracı'nın performansının nasıl kademeli olarak geliştiğini izlememize olanak tanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n",
    "!pip install \"stable-baselines[mpi]==2.10.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a13d8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines.bench import Monitor \n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Eğitim bilgilerini günlüğe kaydetmek için dizin oluştur.\n",
    "log_dir = \"ppo/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# İlerlemeyi günlüğe kaydet\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Vektörize bir ortam oluşturun\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153bd8a",
   "metadata": {},
   "source": [
    "Bir sonraki adım, sinir ağının mimarisini belirlemektir. Bu durumda, konvolüsyonel bir sinir ağı kullanıyoruz.Bunun, her bir sütunu seçme olasılıklarını veren sinir ağı olduğunu unutmayın. PPO algoritmasını kullandığımızdan (aşağıdaki kod hücresinde PPA1), ağımız bazı ek bilgiler de (girişin \"değeri\" olarak adlandırılır) çıkaracaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff90ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import PPO1 \n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc\n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "\n",
    "# Eylem değerlerini tahmin etmek için sinir ağı ( CNN )\n",
    "def modified_cnn(scaled_images, **kwargs):\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = conv_to_fc(layer_2)\n",
    "    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
    "\n",
    "class CustomCnnPolicy(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)\n",
    "        \n",
    "# Agent'ı başlat.\n",
    "model = PPO1(CustomCnnPolicy, vec_env, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d40428",
   "metadata": {},
   "source": [
    "* Yukarıdaki kod hücresinde, sinir ağının ağırlıkları başlangıçta rasgele değerlere ayarlanır.\n",
    "* Ajanın eğitim sırasında aldığı kümülatif ödülün yuvarlanma ortalamasını çiziyoruz. Artan fonksiyonun kanıtladığı gibi, ajan yavaş yavaş oyunu oynayarak daha iyi performans göstermeyi öğrendi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajan'ın eğitimi.\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# toplam ödülü çizelim.\n",
    "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "    firstline = fh.readline()\n",
    "    assert firstline[0] == '#'\n",
    "    df = pd.read_csv(fh, index_col=None)['r']\n",
    "df.rolling(window=1000).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Son olarak, eğitimli ajanı yarışma için gereken formatta belirtiyoruz.\n",
    "\n",
    "def agent1(obs, config):\n",
    "    # Bir sütun seçmek için en iyi modeli kullanın.\n",
    "    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n",
    "    # Seçili sütunun geçerli olup olmadığını kontrol edin.\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # Geçerli değilse rastgele seçin.\n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68943f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oyun zamanı..\n",
    "\n",
    "env = make(\"connectx\")\n",
    "\n",
    "env.run([agent1, \"random\"])\n",
    "\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ve performans hesabı..\n",
    "\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    \n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "             \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "          \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Kazanma:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Kazanma:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Agent 1 Kaybetme:\", outcomes.count([None, 0]))\n",
    "    print(\"Agent 2 Kaybetme:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c17785",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent1, agent2=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd19d78",
   "metadata": {},
   "source": [
    "Daha fazlası için : https://openai.com/blog/competitive-self-play/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
